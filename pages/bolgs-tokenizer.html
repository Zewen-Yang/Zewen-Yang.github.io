<!doctype html>
<html style='font-size:16px !important'>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<link rel="stylesheet" type="text/css" href="../assets/css/dracula.css" />
<title>What is Tokenizer？</title>
</head>
<body class='typora-export'><div class='typora-export-content'>
<div id='write'  class=''><p>&nbsp;</p><div class='md-toc' mdtype='toc'><p class="md-toc-content" role="list"><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n357"><a class="md-toc-inner" style="" href="#what-is-tokenizer">What is Tokenizer?</a></span><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n285"><a class="md-toc-inner" style="" href="#how-many-types-of-tokenizers-mainly-used">How Many Types of Tokenizers Mainly Used?</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n234"><a class="md-toc-inner" style="" href="#word-based-tokenizer">Word-based Tokenizer</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n242"><a class="md-toc-inner" style="" href="#character-based-tokenizer">Character-based Tokenizer</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n251"><a class="md-toc-inner" style="" href="#subword-based-tokenizer">Subword-based Tokenizer</a></span></p></div><p>&nbsp;</p><h1 id='what-is-tokenizer'><span>What is Tokenizer?</span></h1><p><span>In the field of Natural Language Processing (NLP), a tokenizer is a computational tool or component that is responsible for </span><strong><span>segmenting text into smaller units (words or subwords)</span></strong><span> called tokens, which then are converted to </span><strong><span>ids</span></strong><span> through a look-up table. The primary purpose of tokenization is to break down textual data into meaningful chunks that can be easily processed and analyzed by subsequent NLP algorithms and models.</span></p><p><span>Tokenizers play a crucial role in a wide range of NLP applications, such as text classification, named entity recognition, machine translation, and text generation. By dividing text into tokens, tokenizers enable NLP systems to understand and manipulate the underlying linguistic units more effectively.</span></p><p><span>The tokenization process varies depending on the specific requirements of the task and the linguistic characteristics of the language being processed. For instance, in English, tokenization typically involves splitting text based on spaces and punctuation marks, resulting in individual words as tokens. However, tokenization can be more complex in languages where words are often concatenated, such as German, or where there is no clear word boundary, such as Chinese.</span></p><p><span>Tokenizers can be rule-based, employing predefined rules to determine token boundaries, or statistical, utilizing machine learning models that have learned tokenization patterns from large text datasets. Additionally, tokenizers may encompass additional functionalities, such as converting tokens to lowercase, stemming, or lemmatization, to further refine the tokenized output before subsequent analysis or modeling.</span></p><p><span>The quality and accuracy of tokenization significantly impact the performance of downstream NLP tasks and models. Thus, tokenization is a critical preprocessing step in NLP, allowing for effective text processing and enabling the extraction of meaningful information from textual data.</span></p>
<p><img src="../images/blogs/tokenizer/image-20230608141044190.png" referrerpolicy="no-referrer" alt="image-20230608141044190"></p><p>&nbsp;</p><h1 id='how-many-types-of-tokenizers-mainly-used'><span>How Many Types of Tokenizers Mainly Used?</span></h1><p><span>Splitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so. For instance, let’s look at the sentence: &quot;It&#39;s a nice day. Let&#39;s go hiking!&quot;</span></p><p><span>A simple way of tokenizing this text is to split it by spaces, which would give:</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9px; left: 34px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 26px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>1</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -26px; width: 26px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">[<span class="cm-string">"It's"</span>, <span class="cm-string">"a"</span>, <span class="cm-string">"nice"</span>, <span class="cm-string">"day."</span>, <span class="cm-string">"Let's?"</span>, <span class="cm-string">"go"</span>, <span class="cm-string">"hiking!"</span>]</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom-width: 0px; border-bottom-style: solid; border-bottom-color: transparent; top: 22px;"></div><div class="CodeMirror-gutters" style="height: 22px; left: 0px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 26px;"></div></div></div></div></pre><p><span>This is a sensible first step, but if we look at the tokens </span><code>&quot;day.&quot;</code><span> and </span><code>&quot;hiking!&quot;</code><span>, we notice that </span><span style="color:red"><span>the punctuation is attached</span></span><span>  to the words </span><code>&quot;day.&quot;</code><span> and </span><code>&quot;hiking!&quot;</code><span>, which is suboptimal. We should take the punctuation into account so that a model does not have to learn a different representation of a word and every possible punctuation symbol that could follow it, which would explode the number of representations the model has to learn. Taking punctuation into account, tokenizing our exemplary text would give:</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9px; left: 34px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 26px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>1</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -26px; width: 26px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">[<span class="cm-string">"It"</span>, <span class="cm-string">"'s"</span>, <span class="cm-string">"a"</span>, <span class="cm-string">"nice"</span>, <span class="cm-string">"day"</span>, <span class="cm-string">"."</span>, <span class="cm-string">"Let"</span>, <span class="cm-string">"'"</span>, <span class="cm-string">"s"</span> <span class="cm-string">"go"</span>, <span class="cm-string">"hiking"</span>, <span class="cm-string">"!"</span>]</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom-width: 0px; border-bottom-style: solid; border-bottom-color: transparent; top: 22px;"></div><div class="CodeMirror-gutters" style="height: 22px; left: 0px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 26px;"></div></div></div></div></pre><p><span>Better. However, it is disadvantageous, how the tokenization dealt with the word </span><code>&quot;Don&#39;t&quot;</code><span>. </span><code>&quot;Don&#39;t&quot;</code><span> stands for </span><code>&quot;do not&quot;</code><span>, so it would be better tokenized as </span><code>[&quot;Do&quot;, &quot;n&#39;t&quot;]</code><span>. This is where things start getting complicated, and part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.</span></p><ul><li><span>Word-based Tokenizer</span></li><li><span>Character-based Tokenizer</span></li><li><span>Subword-based Tokenizer</span></li></ul><p>&nbsp;</p><h2 id='word-based-tokenizer'><span>Word-based Tokenizer</span></h2><p><span>The first type of tokenizer that comes to mind is </span><em><span>word-based</span></em><span>. It’s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:</span></p>
<p><img src="../images/blogs/tokenizer/WX20230608-140548@2x.png" referrerpolicy="no-referrer" alt="WX20230608-140548@2x"></p><p><a href='https://spacy.io/'><span>spaCy</span></a><span> and </span><a href='http://www.statmt.org/moses/?n=Development.GetStarted'><span>Moses</span></a><span> are two popular </span><strong><span>rule-based tokenizers</span></strong><span>. Applying them on our example, </span><em><span>spaCy</span></em><span> and </span><em><span>Moses</span></em><span> would output something like:</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9px; left: 34px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 26px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>1</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -26px; width: 26px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">[<span class="cm-string">"It"</span>, <span class="cm-string">"'s"</span>, <span class="cm-string">"a"</span>, <span class="cm-string">"nice"</span>, <span class="cm-string">"day"</span>, <span class="cm-string">"."</span>, <span class="cm-string">"Let"</span>, <span class="cm-string">"'s"</span>, <span class="cm-string">"go"</span>, <span class="cm-string">"hiking"</span>, <span class="cm-string">"!"</span>]</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom-width: 0px; border-bottom-style: solid; border-bottom-color: transparent; top: 22px;"></div><div class="CodeMirror-gutters" style="height: 22px; left: 0px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 26px;"></div></div></div></div></pre><p><span>As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. </span><strong><span>Space and punctuation tokenization and rule-based tokenization are both examples of word tokenization</span></strong><span>, which is loosely defined as splitting sentences into words. While it’s the most intuitive way to split texts into smaller chunks, this tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization usually generates a very big vocabulary (the set of all unique words and tokens used). </span><em><span>E.g.</span></em><span>, </span><a href='https://huggingface.co/docs/transformers/model_doc/transformerxl'><span>Transformer XL</span></a><span> uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!</span></p><p><span>Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which causes both an increased memory and time complexity. In general, transformers models rarely have a vocabulary size greater than 50,000, especially if they are pretrained only on a single language.</span></p><div align="center">
    <table border="0" cellspacing="0" cellpadding="0">
     <tbody><tr style="vertical-align:top">
       <td><b>Pros:</b>
          <ul>
          <li>Linguistic Meaning</li>
          <li>Context Preservation</li>
          <li>Readability and Interpretability</li>
        	</ul>
       </td>
       <td><b>Cons:</b>
         <ul>
          <li>Big vocabulary size</li>
          <li>Different ids for the same word, e.g., dog: 14, dogs:15</li>
          <li>Out-of-Vocabulary (OOV) Words with limited vocbulary resulting in loss of information</li>
           <li>Ambiguity and Polysemy</li>
           <li>Data Sparsity and Overgeneration</li>
        	</ul>
       </td>
     </tr>
    </tbody></table>
</div><p>&nbsp;</p><h2 id='character-based-tokenizer'><span>Character-based Tokenizer</span></h2><ul><li><span>Character-based Tokenizer</span></li></ul><p><span>While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for the model to learn meaningful input representations. </span><em><span>E.g.</span></em><span> learning a meaningful context-independent representation for the letter </span><code>&quot;t&quot;</code><span> is much harder than learning a context-independent representation for the word </span><code>&quot;today&quot;</code><span>. Therefore, character tokenization is often accompanied by a loss of performance.</span></p>
<p><img src="../images/blogs/tokenizer/image-20230605182938996.png" referrerpolicy="no-referrer" alt="image-20230605182938996"></p><div align="center">
    <table border="0" cellspacing="0" cellpadding="0">
     <tbody><tr style="vertical-align:top">
       <td><b>Pros:</b>
          <ul>
          <li>Out-of-Vocabulary (OOV) Handling</li>
          <li>Subword Units</li>
          <li>Language Independence</li>
          <li>Reduced Data Sparsity</li>
        	</ul>
       </td>
       <td><b>Cons:</b>
         <ul>
          <li>Increased Sequence Length</li>
          <li>Loss of Word-Level Semantics</li>
          <li>Lack of Contextual Information</li>
           <li>Increased Training and Inference Time</li>
        	</ul>
       </td>
     </tr>
    </tbody></table>
</div><p>&nbsp;</p><h2 id='subword-based-tokenizer'><span>Subword-based Tokenizer</span></h2><ul><li><span>Subword-based Tokenizer</span></li></ul><p><span>So to get the best of both worlds, transformers models use a hybrid between word-level and character-level tokenization called </span><strong><span>subword</span></strong><span> tokenization.</span></p><p><span>Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance </span><code>&quot;annoyingly&quot;</code><span> might be considered a rare word and could be decomposed into </span><code>&quot;annoying&quot;</code><span> and </span><code>&quot;ly&quot;</code><span>. Both </span><code>&quot;annoying&quot;</code><span> and </span><code>&quot;ly&quot;</code><span> as stand-alone subwords would appear more frequently while at the same time the meaning of </span><code>&quot;annoyingly&quot;</code><span> is kept by the composite meaning of </span><code>&quot;annoying&quot;</code><span> and </span><code>&quot;ly&quot;</code><span>. This is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.</span></p><p><span>Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful context-independent representations. In addition, subword tokenization enables the model to process words it has never seen before, by decomposing them into known subwords. </span></p><p>&nbsp;</p><p><span>For instance, the </span><a href='https://huggingface.co/docs/transformers/v4.29.1/en/model_doc/bert#transformers.BertTokenizer'><span>BertTokenizer</span></a><span> tokenizes </span><code>&quot;I have a new GPU!&quot;</code><span> as follows:</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9px; left: 34px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 26px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>5</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -26px; width: 26px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&gt;&gt;&gt;</span> <span class="cm-keyword">from</span> <span class="cm-variable">transformers</span> <span class="cm-keyword">import</span> <span class="cm-variable">BertTokenizer</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 18px;">2</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="" cm-zwsp="">
</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 18px;">3</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&gt;&gt;&gt;</span> <span class="cm-variable">tokenizer</span> <span class="cm-operator">=</span> <span class="cm-variable">BertTokenizer</span>.<span class="cm-property">from_pretrained</span>(<span class="cm-string">"bert-base-uncased"</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 18px;">4</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&gt;&gt;&gt;</span> <span class="cm-variable">tokenizer</span>.<span class="cm-property">tokenize</span>(<span class="cm-string">"I have a new GPU!"</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">5</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">[<span class="cm-string">"i"</span>, <span class="cm-string">"have"</span>, <span class="cm-string">"a"</span>, <span class="cm-string">"new"</span>, <span class="cm-string">"gp"</span>, <span class="cm-string">"##u"</span>, <span class="cm-string">"!"</span>]</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom-width: 0px; border-bottom-style: solid; border-bottom-color: transparent; top: 110px;"></div><div class="CodeMirror-gutters" style="height: 110px; left: 0px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 26px;"></div></div></div></div></pre><p><span>Because we are considering the uncased model, the sentence was lowercased first. We can see that the words </span><code>[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;]</code><span> are present in the tokenizer’s vocabulary, but the word </span><code>&quot;gpu&quot;</code><span> is not. Consequently, the tokenizer splits </span><code>&quot;gpu&quot;</code><span> into known subwords: </span><code>[&quot;gp&quot; and &quot;##u&quot;]</code><span>. </span><code>&quot;##&quot;</code><span> means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).</span></p><p>&nbsp;</p><p><span>As another example, </span><a href='https://huggingface.co/docs/transformers/v4.29.1/en/model_doc/xlnet#transformers.XLNetTokenizer'><span>XLNetTokenizer</span></a><span> tokenizes our previously exemplary text as follows:</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9px; left: 34px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 26px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>5</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -26px; width: 26px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&gt;&gt;&gt;</span> <span class="cm-keyword">from</span> <span class="cm-variable">transformers</span> <span class="cm-keyword">import</span> <span class="cm-variable">XLNetTokenizer</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 18px;">2</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="" cm-zwsp="">
</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 18px;">3</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&gt;&gt;&gt;</span> <span class="cm-variable">tokenizer</span> <span class="cm-operator">=</span> <span class="cm-variable">XLNetTokenizer</span>.<span class="cm-property">from_pretrained</span>(<span class="cm-string">"xlnet-base-cased"</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 18px;">4</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&gt;&gt;&gt;</span> <span class="cm-variable">tokenizer</span>.<span class="cm-property">tokenize</span>(<span class="cm-string">"Don't you love 🤗 Transformers? We sure do."</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -26px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 18px;">5</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">[<span class="cm-string">"▁Don"</span>, <span class="cm-string">"'"</span>, <span class="cm-string">"t"</span>, <span class="cm-string">"▁you"</span>, <span class="cm-string">"▁love"</span>, <span class="cm-string">"▁"</span>, <span class="cm-string">"🤗"</span>, <span class="cm-string">"▁"</span>, <span class="cm-string">"Transform"</span>, <span class="cm-string">"ers"</span>, <span class="cm-string">"?"</span>, <span class="cm-string">"▁We"</span>, <span class="cm-string">"▁sure"</span>, <span class="cm-string">"▁do"</span>, <span class="cm-string">"."</span>]</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom-width: 0px; border-bottom-style: solid; border-bottom-color: transparent; top: 110px;"></div><div class="CodeMirror-gutters" style="height: 110px; left: 0px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 26px;"></div></div></div></div></pre><p><span>We’ll get back to the meaning of those </span><code>&quot;▁&quot;</code><span> when we look at </span><a href='https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece'><span>SentencePiece</span></a><span>. As one can see, the rare word </span><code>&quot;Transformers&quot;</code><span> has been split into the more frequent subwords </span><code>&quot;Transform&quot;</code><span> and </span><code>&quot;ers&quot;</code><span>.</span></p><p>&nbsp;</p><p><span>Unsurprisingly, there are many more techniques out there. To name a few:</span></p><ul><li><span>Byte-Pair Encoding (BPE), e.g. </span><a href='https://huggingface.co/docs/transformers/model_doc/gpt2'><span>GPT-2</span></a><span>, </span><a href='https://huggingface.co/docs/transformers/model_doc/roberta'><span>Roberta</span></a><span>. More advanced pre-tokenization include rule-based tokenization, e.g. </span><a href='https://huggingface.co/docs/transformers/model_doc/xlm'><span>XLM</span></a><span>, </span><a href='https://huggingface.co/docs/transformers/model_doc/flaubert'><span>FlauBERT</span></a><span> which uses Moses for most languages, or </span><a href='https://huggingface.co/docs/transformers/model_doc/gpt'><span>GPT</span></a><span> which uses Spacy and ftfy, to count the frequency of each word in the training corpus.</span></li><li><span>Byte-level BPE, as used in </span><a href='https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf'><span>GPT-2</span></a><span> </span></li><li><span>WordPiece, as used in  </span><a href='https://huggingface.co/docs/transformers/model_doc/bert'><span>BERT</span></a><span>, </span><a href='https://huggingface.co/docs/transformers/model_doc/distilbert'><span>DistilBERT</span></a><span>, and </span><a href='https://huggingface.co/docs/transformers/model_doc/electra'><span>Electra</span></a></li><li><span>SentencePiece or Unigram, as used in several multilingual models, </span><em><span>e.g.</span></em><span> </span><a href='https://huggingface.co/docs/transformers/model_doc/xlm'><span>XLM</span></a><span> uses a specific Chinese, Japanese, and Thai pre-tokenizer, </span><a href='https://huggingface.co/docs/transformers/model_doc/albert'><span>ALBERT</span></a><span>, </span><a href='https://huggingface.co/docs/transformers/model_doc/xlnet'><span>XLNet</span></a><span>, </span><a href='https://huggingface.co/docs/transformers/model_doc/marian'><span>Marian</span></a><span>, and </span><a href='https://huggingface.co/docs/transformers/model_doc/t5'><span>T5</span></a></li></ul><p><span>You should now have sufficient knowledge of how tokenizers work to get started with the API.</span></p></div></div>
</body>
</html>